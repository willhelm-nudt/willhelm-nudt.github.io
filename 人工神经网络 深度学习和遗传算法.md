# 人工神经网络
人工神经网络是由大量简单的基本元件--神经元(neuro)互相连接而成的自适应非线性动态系统。每个神经元的结构功能比较简单，而大量神经元组合产生的系统行为却非常复杂。人工神经元是对生物神经元的简化和模拟，是一个多输入、单输出的非线性元件，起输入输出关系可描述为：

![1.1](https://github.com/willhelm-nudt/photo/blob/master/f.png)

![1.2](https://github.com/willhelm-nudt/photo/blob/master/1.1.png)

_y_<sub>i</sub>=&fnof;(_I_<sub>_i_</sub>)

_j_ 是其他细胞传来的输入信号，θ为神经元的阈值，_w<sub>ji</sub>_ 表示细胞 _j_ 到细胞 _i_ 的连接权值，正负取值，表示激发开关。y<sub>i</sub>为神经元输出，&fnof;(.)叫传递函数，或者叫激励函数，往往采用0和1 二值函数或S函数。


![1.3](https://github.com/willhelm-nudt/photo/blob/master/sigmod.png)
```c
#include <stdio.h>
#include <stdlib.h>

double data1[2]={1,1};
double data2[2]={-1,-1};

double data1_class=1;
double data2_class=0;

double w[2]={0,0};

double b=0;

double sumfun(double *data,double *weight,double bias)
{
	return(data[0]*weight[0]+data[1]*weight[1]+bias);
}

double step(double sum)
{
	if(sum>0)
		return 1;
	else
		return 0;
}

int main(){
	double sum=0;
	double output1=0,output2=0;
	int count=0;
	double err=0;
	int flag1=0,flag2=0;
	while(1){
		sum=sumfun(data1,w,b);
		output1=step(sum);
		if(output1==data1_class)
			flag1=1;
		else{
			flag1=0;
			err=data1_class-output1;
			w[0]=w[0]+err*data1[0];
			w[1]=w[1]+err*data1[1];
			b=b+err;
		}
		sum=sumfun(data2,w,b);
		output2=step(sum);
		if(output2==data2_class)
			flag2=1;
		else
		{
			flag2=0;
			err=data2_class-output2;
			w[0]=w[0]+err*data2[0];
			w[1]=w[1]+err*data2[1];
			b=b+err;
		}
		printf("The %d's training output:\n",count+=1);
		printf("First group data belongs to %1.0f class.\n",output1);
		printf("Second group data belongs to %1.0f class.\n",output2);
		if(flag1==1&&flag2==1)
		{
			break;
		}
	}
	printf("\n\nThe training done!\n\n");
	return 0;

}
```
```
[1] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement learning,” arXiv preprint arXiv:1611.01578, 2016.1. 
[2] Q. Le and B. Zoph, "Using Machine Learning to Explore Neural Network Architecture", Google AI Blog, 2017. [Online]. Available: https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html. [Accessed: 09- Feb- 2020]. 
[3] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable architectures for scalable image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8697–8710. 
[4] H. Cai, L. Zhu, and S. Han, “Proxylessnas: Direct neural architecture search on target task and hardware,” arXiv preprint arXiv:1812.00332, 2018. 
[5] H. Liu, K. Simonyan, and Y. Yang, “Darts:  Differentiable architecture search,” arXiv preprint arXiv:1806.09055, 2018. 
[6] "简述AutoML由来与其应用现状", 知乎, 2019. [Online]. Available: https://zhuanlan.zhihu.com/p/57404166. [Accessed: 09- Feb- 2020]. 
[7] R. Thomas, "An Opinionated Introduction to AutoML and Neural Architecture Search · fast.ai", Fast.ai, 2018. [Online]. Available: https://www.fast.ai/2018/07/16/auto-ml2/#auto-ml. [Accessed: 09- Feb- 2020]. 
```
