# 人工神经网络
人工神经网络是由大量简单的基本元件--神经元(neuro)互相连接而成的自适应非线性动态系统。每个神经元的结构功能比较简单，而大量神经元组合产生的系统行为却非常复杂。人工神经元是对生物神经元的简化和模拟，是一个多输入、单输出的非线性元件，起输入输出关系可描述为：

![1.1](https://github.com/willhelm-nudt/photo/blob/master/f.png)

![1.2](https://github.com/willhelm-nudt/photo/blob/master/1.1.png)

_y_<sub>i</sub>=&fnof;(_I_<sub>_i_</sub>)

_j_ 是其他细胞传来的输入信号，θ为神经元的阈值，_w<sub>ji</sub>_ 表示细胞 _j_ 到细胞 _i_ 的连接权值，正负取值，表示激发开关。y<sub>i</sub>为神经元输出，&fnof;(.)叫传递函数，或者叫激励函数，往往采用0和1 二值函数或S函数。


![1.3](https://github.com/willhelm-nudt/photo/blob/master/sigmod.png)
```c
#include <stdio.h>
#include <stdlib.h>

double data1[2]={1,1};
double data2[2]={-1,-1};

double data1_class=1;
double data2_class=0;

double w[2]={0,0};

double b=0;

double sumfun(double *data,double *weight,double bias)
{
	return(data[0]*weight[0]+data[1]*weight[1]+bias);
}

double step(double sum)
{
	if(sum>0)
		return 1;
	else
		return 0;
}

int main(){
	double sum=0;
	double output1=0,output2=0;
	int count=0;
	double err=0;
	int flag1=0,flag2=0;
	while(1){
		sum=sumfun(data1,w,b);
		output1=step(sum);
		if(output1==data1_class)
			flag1=1;
		else{
			flag1=0;
			err=data1_class-output1;
			w[0]=w[0]+err*data1[0];
			w[1]=w[1]+err*data1[1];
			b=b+err;
		}
		sum=sumfun(data2,w,b);
		output2=step(sum);
		if(output2==data2_class)
			flag2=1;
		else
		{
			flag2=0;
			err=data2_class-output2;
			w[0]=w[0]+err*data2[0];
			w[1]=w[1]+err*data2[1];
			b=b+err;
		}
		printf("The %d's training output:\n",count+=1);
		printf("First group data belongs to %1.0f class.\n",output1);
		printf("Second group data belongs to %1.0f class.\n",output2);
		if(flag1==1&&flag2==1)
		{
			break;
		}
	}
	printf("\n\nThe training done!\n\n");
	return 0;

}
```
```
[1] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement learning,” arXiv preprint arXiv:1611.01578, 2016.1. 
[2] Q. Le and B. Zoph, "Using Machine Learning to Explore Neural Network Architecture", Google AI Blog, 2017. [Online]. Available: https://ai.googleblog.com/2017/05/using-machine-learning-to-explore.html. [Accessed: 09- Feb- 2020]. 
[3] B. Zoph, V. Vasudevan, J. Shlens, and Q. V. Le, “Learning transferable architectures for scalable image recognition,” in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 8697–8710. 
[4] H. Cai, L. Zhu, and S. Han, “Proxylessnas: Direct neural architecture search on target task and hardware,” arXiv preprint arXiv:1812.00332, 2018. 
[5] H. Liu, K. Simonyan, and Y. Yang, “Darts:  Differentiable architecture search,” arXiv preprint arXiv:1806.09055, 2018. 
[6] "简述AutoML由来与其应用现状", 知乎, 2019. [Online]. Available: https://zhuanlan.zhihu.com/p/57404166. [Accessed: 09- Feb- 2020]. 
[7] R. Thomas, "An Opinionated Introduction to AutoML and Neural Architecture Search · fast.ai", Fast.ai, 2018. [Online]. Available: https://www.fast.ai/2018/07/16/auto-ml2/#auto-ml. [Accessed: 09- Feb- 2020]. 
```
# AUTOML

机器学习的应用需要大量的人工干预，这些人工干预表现在：特征提取、模型选择、参数调节等机器学习的各个方面。AutoML 试图将这些与特征、模型、优化、评价有关的重要步骤进行自动化地学习，使得机器学习模型无需人工干预即可被应用。

1. 从机器学习角度讲，AutoML 可以看作是一个在给定数据和任务上学习和泛化能力非常强大的系统。但是它强调必须非常容易使用；

2. 从自动化角度讲，AutoML 则可以看作是设计一系列高级的控制系统去操作机器学习模型，使得模型可以自动化地学习到合适的参数和配置而无需人工干预。 

AutoML 的主要问题可以由三部分构成：特征工程、模型选择、算法选择。

## 特征工程
特征工程在机器学习中有着举足轻重的作用。在 AutoML 中，自动特征工程的目的是自动地发掘并构造相关的特征，使得模型可以有最优的表现。除此之外，还包含一些特定的特征增强方法，例如特征选择、特征降维、特征生成、以及特征编码等。这些步骤目前来说都没有达到自动化的阶段。

上述这些步骤也伴随着一定的参数搜索空间。第一种搜索空间是方法自带的，例如PCA自带降维参数需要调整。第二种是特征生成时会将搜索空间扩大。

## 模型选择

模型选择包括两个步骤：选择一个模型，设定它的参数。相应地，AutoML的目的就是自动选择出一个最合适的模型，并且能够设定好它的最优参数。

## 算法选择

对于算法选择，AutoML 的目的是自动地选择出一个优化算法，以便能够达到效率和精度的平衡。常用的优化方法有 SGD、L-BFGS、GD 等。使用哪个优化算法、对应优化算法的配置，也需要一组搜索空间。

将以上三个关键步骤整合起来看，一个完整的 AutoML 过程可以分成这么两类：一类是将以上的三个步骤整合成一个完整的 **pipeline**；另一类则是 **Network Architecture Search**，能够自动地学习到最优的网络结构。在学习的过程中，对以上三个问题都进行一些优化。

### 优化策略

一旦搜索空间确定，我们便可以实用优化器（optimizer）进行优化。这里，AutoML 主要回答三个问题： 

选择的优化器可以作用在哪个搜索空间上？

它需要什么样的反馈？

为了取得一个好的效果，它需要怎样的配置？ 

